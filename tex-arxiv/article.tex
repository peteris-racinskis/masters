\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{A Motion Capture and Imitation Learning-Based Approach to Robot Control}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
%\date{}

\author{ \href{https://orcid.org/0000-0002-8956-179X}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Peteris Racinskis}\thanks{Robotics and Machine Perception Laboratory} \thanks{Institute of Electronics and Computer Science, Riga, Latvia} \\
	\texttt{peteris.racinskis@edi.lv} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0001-5203-3347}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Janis Arents}\footnote[1]{test} \footnote[2]{test}\\
	\texttt{janis.arents@edi.lv} \\
	\And
	\href{https://orcid.org/0000-0002-5405-0738}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Modris Greitans}\footnote[2]{} \\
	\texttt{modris\_greitans@edi.lv} \\
}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A Motion Capture and Imitation Learning-Based Approach to Robot Control},
pdfsubject={cs.RO},
pdfauthor={Peteris Racinskis, Janis Arents, Modris Greitans},
pdfkeywords={Imitation Learning, Motion capture, Robotics, Artificial neural networks, RNN},
}

\begin{document}
\maketitle

\begin{abstract}
	Imitation Learning is a discipline of Machine Learning primarily concerned with replicating observed behavior of agents known to perform well on a given task, collected in demonstration data sets. In an industrial robotics context this presents the opportunity to replace explicit programming of behavior with demonstrations of the task to be performed. Motion capture is one of the methods for recording such data. It enables lesser model complexity compared to more indirect observation modalities such as visual data, yet requires additional data pre-processing if signals beyond a time series of effector positions and orientations are relevant to the task at hand. In this paper, an approach for motion capture-based imitation learning and implicit control signal estimation is introduced and evaluated on an object throwing task.
\end{abstract}


% keywords can be removed
\keywords{Imitation Learning \and Motion capture \and Robotics \and Artificial neural networks \and RNN}


\section{Introduction}

Manipulator arms and other types of robots have become ubiquitous in modern industry and their use has been proliferating for decades, yet even now the primary method for programming these devices remains procedural code, hand-crafted by specially trained technicians and engineers. This significantly increases the cost and complexity of commissioning process nodes that utilize industrial robots. To this end, various alternative approaches grounded in machine learning have been proposed. Perhaps the most commonly studied are methods that fall under the umbrella of reinforcement learning, which optimize an agent acting on its environment through the use of an explicitly specified reward function \citep{sutton2018reinforcement}. While offering strong theoretical guarantees of convergence, they require interaction with the environment for learning to occur. Furthermore, in practical settings the reward function for many tasks is very sparse, leading to large search spaces and inefficient exploration \citep{hester2018deep}. Finally, the reward function for a task may be unknown or difficult to specify analytically \citep{abbeel2004apprenticeship}.

Imitation learning is a broad field of study in its own right that promises to overcome some or all of the aforementioned challenges, provided that it is possible to obtain a corpus of demonstrations showing how the task is to be accomplished. When dealing with industrial robotics, this is often the case as the tasks we wish to execute are ones that human operators already routinely perform. Therefore, we have developed an approach for recording actions taken by humans in the physical environment, programmatically producing a training data set structured in the form of explicit demonstrations augmented with additional, extrapolated state inputs and control signals, and training artificial neural network models to reproduce the trajectories therein as motion plans.

Given the inherent trade-off between task specificity and required model complexity when it comes to observation data modality and model output, motion capture has been selected as a convenient middle ground. It does not require a simulated environment as approaches that involve virtual reality do \citep{zhang2018deep,dyrstad2018teaching}, obviates any issues that may come with having to learn motions in robot configuration space by operating in Cartesian coordinates, and does not necessitate the use of complex image processing layers as found in models with visual input modalities \citep{liu2018imitation, zhang2018deep}. The main drawbacks of this approach are that it requires motion capture equipment, which is more expensive than conventional video cameras, and Cartesian motion plans need an additional conversion step into joint space trajectories before it is possible to execute them on a real robot. Given the intended application of this system -- programming robots to perform tasks in an industrial environment -- the advantages of compact models, rapid training times and results that are possible to evaluate ahead of time were deemed to outweigh the drawbacks.

To help guide the development process and serve as a means of evaluation, a sample use case was selected -- object throwing. While on the surface the task is almost entirely defined by elementary ballistics that can be programmed explicitly, it serves as a convenient benchmark for robot programming by way of demonstrations. The task was deemed sufficiently non-trivial when considered from the perspective of a Markov Decision Process or time-series model only given limited information about system state at any given time step, while also being suitable for intuitive evaluation by human observers due to its intuitively straightforward nature. Furthermore, the relationship between release position, velocity and target coordinates provides an obvious way to quantitatively evaluate model performance against training and validation data -- extrapolated throw accuracy.


\section{Preliminaries}
\label{sec:prelim}



\section{Related Work}
\label{sec:related}

In its simplest and most straightforward form -- sometimes referred to as \emph{behavioral cloning} -- imitation learning can be reduced to a classification or regression task. Given a set of states and actions or state transitions produced by an unknown expert function, a model (\emph{policy}) is trained to approximate this function. Even with very small parameter counts by modern standards, when combined with artificial neural networks this method has demonstrated some success as far back as the 1980s, provided a task with simple dynamics \citep{pomerleau1989alvinn}.

However, it has since become apparent that pure behavioral cloning suffers from distribution shift -- a phenomenon whereby the distribution of states visited by the policy diverges from that of the original training data set by way of incremental error, eventually leading to poor predictions by the model and irrecoverable deviation from the intended task. To improve the ability of policies to recover from this failure mode, various more complex approaches to the task have been proposed. One major direction of research has been the use of inherently robust, composable functions known as \emph{dynamic motion primitives}, employing systems of differential equations and parametric models such as Gaussian basis functions to obviate the problem of distribution shift entirely -- ensuring convergence towards the goal through the explicit dynamics of the system \citep{pastor2009learning}. Though showing promising results, it must be noted that the model templates used are quite domain-specific. Others have attempted to use interactive sampling with more general-purpose 



\section{Proposed approach}

The overall approach proposed in this paper can be broken down into three main sections -- the collection of observations in the physical environment (\ref{sec:collection}), a pipeline for turning raw observation data into structured demonstrations (\ref{sec:preprocess}) and neural network model implementations (\ref{sec:models}). Two general approaches to system evaluation and design feedback were employed. First, qualitative observations of the generated trajectories were made using spatial visualization in a virtual environment, followed by execution on simulated and real robots (\ref{sec:exec}). Additionally, a series of quantitative metrics have been computed comparing system outputs with training and validation datasets (\ref{sec:eval}).

\section{Implementation}
\label{sec:materials}


\subsection{Data collection}
\label{sec:collection}



\subsection{Pre-processing, extraction of implicit control signals}
\label{sec:preprocess}

bla bla bla

\subsection{Models}
\label{sec:models}

bla bla bla

\subsection{Visualization and execution}
\label{sec:exec}


bla bla bla

\subsection{Evaluation metrics}
\label{sec:eval}

bla bla bla


\section{Results}

bla bla bla


\section{Discussion}

bla bla bla




\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .



\end{document}
