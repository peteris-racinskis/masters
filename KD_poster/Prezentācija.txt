Pēteris Račinskis
pr20015
Labdien, mani sauc Pēteris Račinskis un mans kursa darbs ir par tēmu “Atdarinošās mašīnmācīšanās pielietojums robotikā”. Šī tēma izvēlēta sadarbībā ar Elektronikas un datorzinātņu institūta pētniekiem un darba vadītājs ir direktors Modris Greitāns. Paredzams, ka šis darbs kalpos par pamatu noslēguma darba teorētiskai daļai. Līdz ar to kursadarbs ir primāri literatūras analīze. 
Darba mērķis ir divējāds. 
Vispirms atdarinošā mašīnmācīšanās jeb imitation learning izvēlēta par vispārīgu izpētes virzienu. Šobrīd robotikas laboratorijas pētnieki tajā saskata potenciālu un vēlas būvēt zināšanu bāzi tālākiem pētījumiem. 
Otrkārt, par motivējošo uzdevumu izvēlēta viena konkrēta problēma, ko ar imitāciju varētu risināt. Metienu iestrādāšana industriāla šķirošanas robota trajektroijās. 
Šobrīd aktīvi noris darbība atkritumu šķirošanas līniju automatizācijā, kurā institūts ir iesaistīts. Tieši no turienes arī nākusi ideja par šādu risinājumu. Atdarinošās metodes šķiet aktuālas tieši tāpēc, ka klasiskā stimulētā mašīnmācīšanās - jeb reinforcement learning - sastopas ar lielām grūtībām uzdevumos, kur ir stipri retinātas atalgojuma funkcijas. Šajā gadījumā atalgojuma funkcija būtu atkarīga no tā, vai objekts ir iemests pareizajā tvertnē. Ar nejauši izvēlētām kustībām ir ļoti maza varbūtība nonākt tādā stāvoklī.
Tagad īsi par atdarinošo mācīšanos kopumā. Gan imitation, gan reinforcement learning metodes pamatā nodarbojas ar tādu uzdevumu risināšanu, ko var izteikt ar Markova lēmumu procesiem jeb MDP. MDP sastāv no sistēmas stāvokļiem - st - un darbībām - at. Sistēma ar noteiktu varbūtību pāriet no viena stāvokļa uz citu, atkarībā no pašreizējā stāvokļa un izvēlētās darbības. MDP galvenā īpašība ir tā, ka nekas nav jāzina par iepriekšējiem stāvokļiem - nākotni var pilnībā paredzēt, zinot tikai pašreizējos procesa parametrus. Robota kontekstā stāvoklis parasti ir paša robota un citu objektu pozīcijas, bet darbības ir kustības.
Ar mašīnmācīšanās metodēm var apmācīt modeļus - stratēģijas - kas atkarībā no stāvokļa izvēlas nākamo darbību. reinforcement learning metodes izmanto atalgojuma funkciju, lai novērtētu cik katrs stāvoklis ir labs vai slikts, bet imitation learning pamatā ir ierakstītu demonstrāciju izmantošana. Katra demonstrācija var sastāvēt no stāvokļiem, tiešiem vai netiešiem novērojumiem, kā arī stāvokļu-darbību vai novērojumu-darbību pāriem. 
Novērojumus sauc par tiešiem, ja tos tiešā veidā var izmantot procesa vadībai - piemēram, robota iekšējo stāvokli un manipulējamā objekta koordinātes. Netiešiem novērojumiem nepieciešama papildus apstrāde, lai iegūtu noderīgu informāciju - tādi var būt, piemēram, attēli no videokameras.
Tā kā šis ir diezgan plašs izpētes lauks, literatūras analīzes gaitā tika izdalīti trīs galvenie virzieni, kuros var iedalīt līdz šim veiktos pētījumus. Katrā no tiem apskatīti pētījumi, kas provizoriski šķiet, ka varētu noderēt risinājuma izstrādē.
Pirmais virziens ir trajektoriju kopēšana - varētu teikt atdarinošā mācīšanās tās tiešākajā nozīmē. Šeit pētīti veidi, kā precīzāk un efektīvāk atdarināt trajektorijas, ja viss, kas vajadzīgs, jau ir zināms - pieejami tiešie novērojumi, zināmas darbības un var pieņemt, ka demonstrācijas ir pietiekami labas. Pamatā visam ir uzvedības klonēšanas metodes - ar demonstrāciju apmāca klasifikatoru vai regresoru, kas paredz nākamo darbību. Taču šeit sastopami dažādi izaicinājumi, galvenokārt tā saucamais distribution shift. Rupji runājot, imitējošais modelis nonāk stāvokļos, kas demonstrāciju kopā nav nosegti, un nezina kā reaģēt. To risina ar interaktīvām korekcijām, uzdevumu sadalīšanu vienkāršākos apakšposmos, inverso stimulēto mācīšanos un ģeneratīviem pretinieku tīkliem.
Otrs lielais virziens ir novērojumu iegūšana un papildināšana. Ar to risina situācijas, kad: 
* pieejami netiešie novērojumie - piemēram, video; 
* vai nav zināmas darbības; 
* vai arī datu apjoms ir nepietiekams; 
* vai arī darba apstākļi - piemēram, manipulējamo objektu masa - ir mainīgi. 
Šajā kontekstā izstrādāti tādi risinājumi, kā: modeļi kas iemācās sistēmas dinamiku, lai paredzētu pareizās darbības; vai autoenkoderi, kas ļauj izteikt iekodēt sistēmas stāvokli un pārnest to no vienas vizuālas perspektīvas uz citu.
Pēdējais virziens, kas tika izdalīts, ir demonstrāciju vispārināšana un adaptācija. Šeit tiek apskatīti gadījumi, kad jāuzlabo neoptimālas demonstrācijas vai uzreiz jāatdarina darbības no vienas demonstrācijas. Neoptimālu demonstrāciju uzlabošanu parasti veic, kombinējot atdarināšanu ar reinforcement learning. Piemēram, izmantojot demonstrācijas kā inicializāciju mācīšanās procesam. Tūlītēju atdarināšanu realizē, apmācot modeli, kas spēj vispārīgi paredzēt ceļu no viena stāvokļa uz citu. Argumentā ņemot vai nu veselu demonstrāciju, vai vēlamo galamērķi.
Risinājumu plānots izstrādāt nākamā semestra gaitā institūta robotikas laboratorijā. Objekta satveršanas un klasifikācijas uzdevumus šobrīd var uzskatīt par atrisinātiem, tāpēc sagaidāms, ka lielākais darbs būs demonstrāciju ieguvē un modeļa būvēšanā. Demonstrācijas plānots iegūt ar motion tracking kameras vai virtuālās realitātes palīdzību - aprīkojums šiem mērķiem ir pieejams. Modelis tiks veidots, izmantojot vispārināmu atdarināšanu, kur kā parametrs tiek ņemts galamērķis - lai nebūtu katrreiz pārkārtojot darba telpu pa jaunam jāveic apmācība. Paredzēts arī izmantot reinforcement learning, lai optimizētu iegūto stratēģiju. Tam visam papildus būs nepieciešams izstrādāt arī testu protokolu, lai eksperimentāli pārbaudītu iegūto rezultātu un salīdzinātu ar alternatīvām.